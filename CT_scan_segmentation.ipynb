{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nibabel\n",
        "!pip install SimpleITK"
      ],
      "metadata": {
        "id": "BnUZ0wbc65MK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bebfaf38-d5d0-47e4-f863-c45049e92564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (5.3.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel) (6.5.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from nibabel) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel) (4.12.2)\n",
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.4.1-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Downloading SimpleITK-2.4.1-cp311-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import nibabel as nib\n",
        "from sklearn.model_selection import train_test_split\n",
        "import SimpleITK as sitk\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "AAcI2RW_ub88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "tFK9Nl8Euc1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "669ebbe1-e56a-4c3e-b22a-516defcf2cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions\n",
        "def pad_to_shape(this, shp):\n",
        "    if len(shp) == 4:\n",
        "        pad = (0, shp[3] - this.shape[3], 0, shp[2] - this.shape[2])\n",
        "    elif len(shp) == 5:\n",
        "        pad = (0, shp[4] - this.shape[4], 0, shp[3] - this.shape[3], 0, shp[2] - this.shape[2])\n",
        "    return F.pad(this, pad)\n",
        "\n",
        "def calculate_dice_score(pred_mask, gt_mask):\n",
        "    intersection = torch.sum(pred_mask * gt_mask)\n",
        "    total_pixels = torch.sum(pred_mask) + torch.sum(gt_mask)\n",
        "    dice = (2.0 * intersection) / (total_pixels + 1e-8)  # Adding a small epsilon to avoid division by zero\n",
        "    return dice\n",
        "\n",
        "def dice_score(y_pred_bin, y_true):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        y_pred_bin: shape => (batch_size, 1, h, w, d)\n",
        "        y_true: shape => (batch_size, 1, h, w, d)\n",
        "\n",
        "    Returns:\n",
        "        : shape => (batch_size, dice_score)\n",
        "    \"\"\"\n",
        "    dice_scores = []\n",
        "    for pred_mask, gt_mask in zip(y_pred_bin, y_true):\n",
        "        dice = calculate_dice_score(pred_mask, gt_mask)\n",
        "        dice_scores.append(dice)\n",
        "    return dice_scores"
      ],
      "metadata": {
        "id": "zGrb-zQlurx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loading Functions\n",
        "def load_nii_data(images_dir, labels_dir, max_samples=None):\n",
        "    \"\"\"Load .nii CT scan data and corresponding labels\"\"\"\n",
        "    image_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.nii') or f.endswith('.nii.gz')])\n",
        "    label_files = sorted([f for f in os.listdir(labels_dir) if f.endswith('.nii') or f.endswith('.nii.gz')])\n",
        "\n",
        "    if max_samples:\n",
        "        image_files = image_files[:max_samples]\n",
        "        label_files = label_files[:max_samples]\n",
        "\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    print(\"Loading NII data...\")\n",
        "    for img_file, lbl_file in tqdm(zip(image_files, label_files), total=len(image_files)):\n",
        "        # Load image and label\n",
        "        img_path = os.path.join(images_dir, img_file)\n",
        "        lbl_path = os.path.join(labels_dir, lbl_file)\n",
        "\n",
        "        img_nii = nib.load(img_path)\n",
        "        lbl_nii = nib.load(lbl_path)\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        img_data = img_nii.get_fdata()\n",
        "        lbl_data = lbl_nii.get_fdata()\n",
        "\n",
        "        # Ensure label is binary (liver segmentation)\n",
        "        lbl_data = (lbl_data > 0).astype(np.float32)\n",
        "\n",
        "        # Add channel dimension and convert to torch tensors\n",
        "        img_tensor = np.expand_dims(img_data, axis=0)\n",
        "        lbl_tensor = np.expand_dims(lbl_data, axis=0)\n",
        "\n",
        "        images.append(img_tensor)\n",
        "        labels.append(lbl_tensor)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "def preprocess_data(images, labels, target_shape=(1, 128, 128, 128)):\n",
        "    \"\"\"Preprocess data - resizing, normalizing, etc.\"\"\"\n",
        "    processed_images = []\n",
        "    processed_labels = []\n",
        "\n",
        "    print(\"Preprocessing data...\")\n",
        "    for img, lbl in tqdm(zip(images, labels), total=len(images)):\n",
        "        # Get original shape\n",
        "        orig_shape = img.shape\n",
        "\n",
        "        # Resize if necessary (using simple cropping/padding for demonstration)\n",
        "        # In production, consider proper interpolation methods\n",
        "        if orig_shape[1:] != target_shape[1:]:\n",
        "            # Crop or pad\n",
        "            cropped_img = img[:,\n",
        "                             :min(orig_shape[1], target_shape[1]),\n",
        "                             :min(orig_shape[2], target_shape[2]),\n",
        "                             :min(orig_shape[3], target_shape[3])]\n",
        "\n",
        "            padded_img = np.zeros(target_shape)\n",
        "            padded_img[:,\n",
        "                      :cropped_img.shape[1],\n",
        "                      :cropped_img.shape[2],\n",
        "                      :cropped_img.shape[3]] = cropped_img\n",
        "\n",
        "            cropped_lbl = lbl[:,\n",
        "                             :min(orig_shape[1], target_shape[1]),\n",
        "                             :min(orig_shape[2], target_shape[2]),\n",
        "                             :min(orig_shape[3], target_shape[3])]\n",
        "\n",
        "            padded_lbl = np.zeros(target_shape)\n",
        "            padded_lbl[:,\n",
        "                      :cropped_lbl.shape[1],\n",
        "                      :cropped_lbl.shape[2],\n",
        "                      :cropped_lbl.shape[3]] = cropped_lbl\n",
        "\n",
        "            processed_images.append(padded_img)\n",
        "            processed_labels.append(padded_lbl)\n",
        "        else:\n",
        "            processed_images.append(img)\n",
        "            processed_labels.append(lbl)\n",
        "\n",
        "    return np.array(processed_images), np.array(processed_labels)"
      ],
      "metadata": {
        "id": "7IzSy1npuusg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Evaluation Functions\n",
        "def one_epoch(model, loader, criterion, optimizer, scheduler, device, samples_count, phase):\n",
        "  if phase == 'train':\n",
        "    model.train()  # Set model to training mode\n",
        "  else:\n",
        "    model.eval()\n",
        "\n",
        "  running_loss = 0.0\n",
        "  running_dice = 0.0\n",
        "\n",
        "  # Iterate over data.\n",
        "  for inputs, labels, indices in loader:\n",
        "    inputs = inputs.type(torch.FloatTensor).to(device)\n",
        "    labels = labels.type(torch.LongTensor).to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward\n",
        "    with torch.set_grad_enabled(phase == 'train'):\n",
        "      outputs = model(inputs)\n",
        "      _, preds = torch.max(outputs, 1, keepdim=True)\n",
        "      loss = criterion(outputs, labels[:,0])\n",
        "\n",
        "      if phase == 'train':\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # statistics\n",
        "    running_loss += loss.item()\n",
        "    running_dice += torch.sum(torch.stack(dice_score(preds, labels)))\n",
        "\n",
        "    if phase == 'train' and scheduler:\n",
        "        scheduler.step()\n",
        "\n",
        "  loss = running_loss / len(loader)\n",
        "  dice = running_dice / samples_count[phase]\n",
        "\n",
        "  return loss, dice.cpu()\n",
        "\n",
        "def train(model, loaders, criterion, optimizer, num_epochs, device, model_path, samples_count, scheduler=None):\n",
        "  best_valid_loss = float('inf')\n",
        "  best_valid_dice = 0\n",
        "\n",
        "  dice_dic, loss_dic = {}, {}\n",
        "  loss_dic['train'], loss_dic['valid'] = [], []\n",
        "  dice_dic['train'], dice_dic['valid'] = [], []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      train_loss, train_dice = one_epoch(model, loaders['train'], criterion, optimizer, scheduler, device, samples_count, phase='train')\n",
        "      val_loss, val_dice = one_epoch(model, loaders['valid'], criterion, optimizer, scheduler, device, samples_count, phase='valid')\n",
        "\n",
        "      loss_dic['train'].append(train_loss)\n",
        "      loss_dic['valid'].append(val_loss)\n",
        "      dice_dic['train'].append(train_dice)\n",
        "      dice_dic['valid'].append(val_dice)\n",
        "\n",
        "      if val_dice > best_valid_dice:\n",
        "        best_valid_dice = val_dice\n",
        "        best_valid_loss = val_loss\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
        "            f'Train Loss: {train_loss:.4f} - '\n",
        "            f'Train Dice: {train_dice:.4f} - '\n",
        "            f'Valid Loss: {val_loss:.4f} - '\n",
        "            f'Valid Dice {val_dice:.4f}')\n",
        "\n",
        "  return loss_dic, dice_dic\n",
        "\n",
        "def evaluate(model, loaders, criterion, optimizer, device, samples_count, phase, scheduler=None):\n",
        "  test_loss, test_dice = one_epoch(model, loaders[phase], criterion, optimizer, scheduler, device, samples_count, phase)\n",
        "  print(f'Test Loss: {test_loss:.4f} - '\n",
        "        f'Test Dice {test_dice:.4f}')\n",
        "  return test_dice\n",
        "\n",
        "def show_plots(num_epochs, data, metric):\n",
        "  e = np.arange(num_epochs)\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(e, data['train'], label='train '+metric)\n",
        "  plt.plot(e, data['valid'], label='validation '+metric)\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend()\n",
        "  plt.savefig(f'{metric}_plot.png')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "S0wcMmx8uypE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "class First3D(nn.Module):\n",
        "    def __init__(self, in_channels, middle_channels, out_channels, dropout=False):\n",
        "        super(First3D, self).__init__()\n",
        "\n",
        "        layers = [\n",
        "            nn.Conv3d(in_channels, middle_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(middle_channels, track_running_stats=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(middle_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels, track_running_stats=False),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "\n",
        "        if dropout:\n",
        "            assert 0 <= dropout <= 1, 'dropout must be between 0 and 1'\n",
        "            layers.append(nn.Dropout3d(p=dropout))\n",
        "\n",
        "        self.first = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.first(x)\n",
        "\n",
        "class Encoder3D(nn.Module):\n",
        "    def __init__(\n",
        "            self, in_channels, middle_channels, out_channels,\n",
        "            dropout=False, downsample_kernel=2\n",
        "    ):\n",
        "        super(Encoder3D, self).__init__()\n",
        "\n",
        "        layers = [\n",
        "            nn.MaxPool3d(kernel_size=downsample_kernel),\n",
        "            nn.Conv3d(in_channels, middle_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(middle_channels, track_running_stats=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(middle_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels, track_running_stats=False),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "\n",
        "        if dropout:\n",
        "            assert 0 <= dropout <= 1, 'dropout must be between 0 and 1'\n",
        "            layers.append(nn.Dropout3d(p=dropout))\n",
        "\n",
        "        self.encoder = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "class Center3D(nn.Module):\n",
        "    def __init__(self, in_channels, middle_channels, out_channels, deconv_channels, dropout=False):\n",
        "        super(Center3D, self).__init__()\n",
        "\n",
        "        layers = [\n",
        "            nn.MaxPool3d(kernel_size=2),\n",
        "            nn.Conv3d(in_channels, middle_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(middle_channels, track_running_stats=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(middle_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels, track_running_stats=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose3d(out_channels, deconv_channels, kernel_size=2, stride=2)\n",
        "        ]\n",
        "\n",
        "        if dropout:\n",
        "            assert 0 <= dropout <= 1, 'dropout must be between 0 and 1'\n",
        "            layers.append(nn.Dropout3d(p=dropout))\n",
        "\n",
        "        self.center = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.center(x)\n",
        "\n",
        "class Decoder3D(nn.Module):\n",
        "    def __init__(self, in_channels, middle_channels, out_channels, deconv_channels, dropout=False):\n",
        "        super(Decoder3D, self).__init__()\n",
        "\n",
        "        layers = [\n",
        "            nn.Conv3d(in_channels, middle_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(middle_channels, track_running_stats=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(middle_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels, track_running_stats=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose3d(out_channels, deconv_channels, kernel_size=2, stride=2)\n",
        "        ]\n",
        "\n",
        "        if dropout:\n",
        "            assert 0 <= dropout <= 1, 'dropout must be between 0 and 1'\n",
        "            layers.append(nn.Dropout3d(p=dropout))\n",
        "\n",
        "        self.decoder = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(x)\n",
        "\n",
        "class Last3D(nn.Module):\n",
        "    def __init__(self, in_channels, middle_channels, out_channels, softmax=False):\n",
        "        super(Last3D, self).__init__()\n",
        "\n",
        "        layers = [\n",
        "            nn.Conv3d(in_channels, middle_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(middle_channels, track_running_stats=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(middle_channels, middle_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(middle_channels, track_running_stats=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(middle_channels, out_channels, kernel_size=1),\n",
        "        ]\n",
        "\n",
        "        self.first = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.first(x)\n",
        "\n",
        "class UNet3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, conv_depths=(16, 32, 64, 128, 256)):\n",
        "        assert len(conv_depths) > 2, 'conv_depths must have at least 3 members'\n",
        "\n",
        "        super(UNet3D, self).__init__()\n",
        "\n",
        "        # defining encoder layers\n",
        "        encoder_layers = []\n",
        "        encoder_layers.append(First3D(in_channels, conv_depths[0], conv_depths[0]))\n",
        "        encoder_layers.extend([Encoder3D(conv_depths[i], conv_depths[i + 1], conv_depths[i + 1])\n",
        "                               for i in range(len(conv_depths)-2)])\n",
        "\n",
        "        # defining decoder layers\n",
        "        decoder_layers = []\n",
        "        decoder_layers.extend([Decoder3D(2 * conv_depths[i + 1], 2 * conv_depths[i], 2 * conv_depths[i], conv_depths[i])\n",
        "                               for i in reversed(range(len(conv_depths)-2))])\n",
        "        decoder_layers.append(Last3D(conv_depths[1], conv_depths[0], out_channels))\n",
        "\n",
        "        # encoder, center and decoder layers\n",
        "        self.encoder_layers = nn.ModuleList(encoder_layers)\n",
        "        self.center = Center3D(conv_depths[-2], conv_depths[-1], conv_depths[-1], conv_depths[-2])\n",
        "        self.decoder_layers = nn.ModuleList(decoder_layers)\n",
        "\n",
        "    def forward(self, x, return_all=False):\n",
        "        # Store intermediates for skip connections\n",
        "        skip_connections = []\n",
        "\n",
        "        # Encode\n",
        "        x = self.encoder_layers[0](x)\n",
        "        skip_connections.append(x)\n",
        "\n",
        "        for i in range(1, len(self.encoder_layers)):\n",
        "            x = self.encoder_layers[i](x)\n",
        "            skip_connections.append(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.center(x)\n",
        "\n",
        "        # Decode with skip connections\n",
        "        for i in range(len(self.decoder_layers)-1):\n",
        "            skip = skip_connections[-(i+1)]\n",
        "            x = torch.cat((x, skip), dim=1)\n",
        "            x = self.decoder_layers[i](x)\n",
        "\n",
        "        # Final layer\n",
        "        skip = skip_connections[0]\n",
        "        x = torch.cat((x, skip), dim=1)\n",
        "        x = self.decoder_layers[-1](x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "77Ei_K5Tu1Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset3D(Dataset):\n",
        "    def __init__(self, x, y, normalization=True):\n",
        "        self.normalization = normalization\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)  # number of samples\n",
        "\n",
        "    def __getitem__(self, index):  # sampling method. used by DataLoader.\n",
        "        x = self.x[index]\n",
        "        y = self.y[index]\n",
        "        if self.normalization:\n",
        "            # Normalize per volume\n",
        "            x = (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
        "        return x, y, index  # we return the index as well for future use"
      ],
      "metadata": {
        "id": "3xEAboTeu4tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, image, device):\n",
        "    \"\"\"\n",
        "    Predict segmentation for a single 3D image\n",
        "    \"\"\"\n",
        "    # Add batch dimension if not present\n",
        "    if len(image.shape) == 4:\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "\n",
        "    input_tensor = torch.from_numpy(image).type(torch.float).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        _, pred = torch.max(output, 1, keepdim=True)\n",
        "\n",
        "    return pred.cpu().numpy()\n",
        "\n",
        "def save_prediction_as_nii(prediction, reference_nii_path, output_path):\n",
        "    \"\"\"\n",
        "    Save a prediction array as a .nii file using the reference file's metadata\n",
        "    \"\"\"\n",
        "    # Load reference nii to get affine transformation and header\n",
        "    ref_nii = nib.load(reference_nii_path)\n",
        "\n",
        "    # Remove batch dimension if present\n",
        "    if len(prediction.shape) == 5:\n",
        "        prediction = prediction[0]\n",
        "\n",
        "    # Remove channel dimension\n",
        "    if prediction.shape[0] == 1:\n",
        "        prediction = prediction[0]\n",
        "\n",
        "    # Create new nii file\n",
        "    pred_nii = nib.Nifti1Image(prediction.astype(np.int32), ref_nii.affine, ref_nii.header)\n",
        "    nib.save(pred_nii, output_path)\n",
        "    print(f\"Saved prediction to {output_path}\")\n",
        "\n",
        "def get_slice_indices(img, k):\n",
        "    \"\"\"Get random slice indices that contain some information\"\"\"\n",
        "    slices = img.shape[1]\n",
        "    indices = []\n",
        "    i = 0\n",
        "    while i < k:\n",
        "        idx = random.randint(0, slices-1)\n",
        "        rnd_slice = img[:, idx]\n",
        "\n",
        "        # Check if slice contains information\n",
        "        if not np.all(rnd_slice == 0):\n",
        "            indices.append(idx)\n",
        "            i += 1\n",
        "    return indices\n",
        "\n",
        "def visualize_results(image, mask, prediction, num_slices=4):\n",
        "    \"\"\"Visualize image, mask and prediction\"\"\"\n",
        "    slices = image.shape[1]\n",
        "    slice_indices = get_slice_indices(mask, num_slices)\n",
        "\n",
        "    fig, axs = plt.subplots(3, num_slices, figsize=(4*num_slices, 12))\n",
        "\n",
        "    for i, s in enumerate(slice_indices):\n",
        "        axs[0][i].title.set_text(f\"Prediction slice {s}\")\n",
        "        axs[0][i].imshow(prediction[0, s, 0], cmap='bone')\n",
        "\n",
        "        axs[1][i].title.set_text(f\"Ground truth slice {s}\")\n",
        "        axs[1][i].imshow(mask[0, s, 0], cmap='bone')\n",
        "\n",
        "        axs[2][i].title.set_text(f\"Original image slice {s}\")\n",
        "        axs[2][i].imshow(image[0, s, 0], cmap='bone')\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.savefig('visualization_results.png')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "D0-0ls9Ru8l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "# Set paths\n",
        "images_dir = \"/content/drive/MyDrive/LITS/imagesTr\"\n",
        "labels_dir = \"/content/drive/MyDrive/LITS/labelsTr\"\n",
        "model_path = \"/content/drive/MyDrive/LITS/Models/model3d.pt\"\n",
        "output_dir = \"/content/drive/MyDrive/LITS/predictions\"\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load and preprocess data\n",
        "raw_images, raw_labels = load_nii_data(images_dir, labels_dir)\n",
        "processed_images, processed_labels = preprocess_data(raw_images, raw_labels)\n",
        "\n",
        "# Split data into train, validation and test sets\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "   processed_images, processed_labels, test_size=0.15, random_state=42\n",
        ")\n",
        "\n",
        "train_X, valid_X, train_Y, valid_Y = train_test_split(\n",
        "   X_train_val, y_train_val, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "test_X, test_Y = X_test, y_test\n",
        "\n",
        "print(f\"Training samples: {len(train_X)}\")\n",
        "print(f\"Validation samples: {len(valid_X)}\")\n",
        "print(f\"Test samples: {len(test_X)}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader3d = DataLoader(\n",
        "   Dataset3D(train_X, train_Y, normalization=True),\n",
        "   batch_size=1,\n",
        "   shuffle=True,\n",
        "   num_workers=2\n",
        ")\n",
        "print('Train Loader Done')\n",
        "\n",
        "valid_loader3d = DataLoader(\n",
        "   Dataset3D(valid_X, valid_Y, normalization=True),\n",
        "   batch_size=1,\n",
        "   shuffle=False,\n",
        "   num_workers=2\n",
        ")\n",
        "print('Validation Loader Done')\n",
        "\n",
        "test_loader3d = DataLoader(\n",
        "   Dataset3D(test_X, test_Y, normalization=True),\n",
        "   batch_size=1,\n",
        "   shuffle=False,\n",
        "   num_workers=2\n",
        ")\n",
        "print('Test Loader Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "fx1-mpzGu-2_",
        "outputId": "83763548-332c-4596-fce7-1d5d533f2216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading NII data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 21/123 [02:20<11:24,  6.71s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5b5eaa85cf12>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load and preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mraw_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_nii_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprocessed_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-2a75d3b186d7>\u001b[0m in \u001b[0;36mload_nii_data\u001b[0;34m(images_dir, labels_dir, max_samples)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Ensure label is binary (liver segmentation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mlbl_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlbl_data\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Add channel dimension and convert to torch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/memmap.py\u001b[0m in \u001b[0;36m__array_wrap__\u001b[0;34m(self, arr, context)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up sample counts and dataloaders\n",
        "train_samples_count3d = len(train_loader3d.dataset)\n",
        "val_samples_count3d = len(valid_loader3d.dataset)\n",
        "test_samples_count3d = len(test_loader3d.dataset)\n",
        "\n",
        "samples_count3d = {\n",
        "   'train': train_samples_count3d,\n",
        "   'valid': val_samples_count3d,\n",
        "   'test': test_samples_count3d\n",
        "}\n",
        "\n",
        "dataloaders3d = {\n",
        "   'train': train_loader3d,\n",
        "   'valid': valid_loader3d,\n",
        "   'test': test_loader3d\n",
        "}\n",
        "\n",
        "# Initialize model, optimizer and loss function\n",
        "model3d = UNet3D(in_channels=1, out_channels=2)\n",
        "model3d = model3d.to(device).float()\n",
        "optimizer = torch.optim.Adam(model3d.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 30\n",
        "\n",
        "# Train model\n",
        "loss_dic, dice_dic = train(model3d, dataloaders3d, criterion, optimizer, epochs, device, model_path, samples_count3d)"
      ],
      "metadata": {
        "id": "FWBaPRSevQeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot results\n",
        "show_plots(epochs, loss_dic, 'loss')\n",
        "show_plots(epochs, dice_dic, 'dice score')"
      ],
      "metadata": {
        "id": "bO16MdLR6zxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model and evaluate\n",
        "model3d.load_state_dict(torch.load(model_path))\n",
        "test_dice = evaluate(model3d, dataloaders3d, criterion, optimizer, device, samples_count3d, 'test')\n",
        "\n",
        "# Make prediction on a test sample and save as .nii\n",
        "test_idx = random.randint(0, len(test_X)-1)\n",
        "test_image = test_X[test_idx]\n",
        "test_mask = test_Y[test_idx]\n",
        "\n",
        "# Get original file path for reference\n",
        "test_file = sorted([f for f in os.listdir(images_dir) if f.endswith('.nii') or f.endswith('.nii.gz')])[test_idx]\n",
        "reference_path = os.path.join(images_dir, test_file)\n",
        "\n",
        "# Predict and save\n",
        "prediction = predict(model3d, test_image, device)\n",
        "output_path = os.path.join(output_dir, f\"prediction_{test_file}\")\n",
        "save_prediction_as_nii(prediction, reference_path, output_path)\n",
        "\n",
        "# Visualize results\n",
        "visualize_results(test_image, test_mask, prediction)\n",
        "\n",
        "print(f\"Final Test Dice Score: {test_dice:.4f}\")\n",
        "print(f\"Prediction saved to {output_path}\")"
      ],
      "metadata": {
        "id": "0dA-nPgp603q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BmiFFDHp9LfD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}